{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We connect to drive here. My data is Saved in My Drive/Course Work/questions.csv\n"
      ],
      "metadata": {
        "id": "kr9IjOK9eTB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HZysmBQUPgA",
        "outputId": "fb80a993-d9ad-468a-c658-881404794df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "isnull is used to check for any missing value"
      ],
      "metadata": {
        "id": "FVlkfN6xedpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft3nDjyPUJei",
        "outputId": "e6400766-a58e-4087-c371-21382123d4ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  qid1  qid2                                          question1  \\\n",
            "0   0     1     2  What is the step by step guide to invest in sh...   \n",
            "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
            "2   2     5     6  How can I increase the speed of my internet co...   \n",
            "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
            "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
            "\n",
            "                                           question2  is_duplicate  \n",
            "0  What is the step by step guide to invest in sh...             0  \n",
            "1  What would happen if the Indian government sto...             0  \n",
            "2  How can Internet speed be increased by hacking...             0  \n",
            "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
            "4            Which fish would survive in salt water?             0  \n",
            "id              0\n",
            "qid1            0\n",
            "qid2            0\n",
            "question1       1\n",
            "question2       2\n",
            "is_duplicate    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/questions.csv')\n",
        "print(data.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing\n",
        "1. lower the text\n",
        "2. remove special characters and numbers\n",
        "3. tokenize: splitting text into individual words or tokens.\n",
        "4. Stemming: It cuts off prefixes and/or endings of words based on common ones. It's faster but less accurate. For example, \"running\" → \"run\".\n",
        "Lemmatization: This involves a more sophisticated analysis to accurately convert words to their base form. For example, \"better\" → \"good\".\n"
      ],
      "metadata": {
        "id": "DlzOYMKPei2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
        "\n",
        "    # Reconstruct the text from tokens\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return processed_text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHM2HpJ6US_v",
        "outputId": "7320fd73-85e8-4eaa-8efb-6906b9a75e1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['question1'] = data['question1'].apply(preprocess_text)\n",
        "data['question2'] = data['question2'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "lhXcPKreVMtL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vectorizer\n",
        "1. we combine the data (q1 and q2)\n",
        "2.CountVectorizer:feature extraction method from the scikit-learn library.It converts a collection of text documents into a matrix of token counts. Essentially, it counts the number of times each word appears in the document.CountVectorizer automates uses BoW. When you fit a CountVectorizer to a corpus of text documents, it performs these steps:\n",
        "\n",
        "It tokenizes the text by separating the words.\n",
        "It builds a vocabulary of unique words. In this vocabulary, each word is assigned a unique index.\n",
        "It transforms each document into a numerical vector. Each element in this vector corresponds to a word in the vocabulary and the value is the count of that word in the document.\n"
      ],
      "metadata": {
        "id": "VdpDNQJofMWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Combine questions\n",
        "data['combined_questions'] = data['question1'] + ' ' + data['question2']\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "# vectorizer = TfidfVectorizer()\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(data['combined_questions'])\n",
        "\n",
        "y = data['is_duplicate']\n"
      ],
      "metadata": {
        "id": "N35zdTA4VWIn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy6B2t4XVeFs",
        "outputId": "a24c3054-3d4e-42c1-d90b-f56bf5730a4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.79      0.78     50998\n",
            "           1       0.63      0.61      0.62     29873\n",
            "\n",
            "    accuracy                           0.73     80871\n",
            "   macro avg       0.71      0.70      0.70     80871\n",
            "weighted avg       0.72      0.73      0.73     80871\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9Sd8LaTWhnX",
        "outputId": "6afa4035-75f3-4b6e-896e-4a570bfd7370"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(323480, 90293)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack"
      ],
      "metadata": {
        "id": "nwJ5rJ82Wkb2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data['question1'] = data['question1'].apply(preprocess_text)\n",
        "data['question2'] = data['question2'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "XETEpv1gbJnV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer1 = CountVectorizer()\n",
        "# vectorizer2 = CountVectorizer()\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer1 = TfidfVectorizer()\n",
        "vectorizer2 = TfidfVectorizer()\n",
        "# Fit and transform the questions\n",
        "X1 = vectorizer1.fit_transform(data['question1'])\n",
        "X2 = vectorizer2.fit_transform(data['question2'])"
      ],
      "metadata": {
        "id": "7Tj816aCbW61"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the absolute difference in lengths of questions\n",
        "data['length_diff'] = abs(data['question1'].str.split().str.len() - data['question2'].str.split().str.len())\n"
      ],
      "metadata": {
        "id": "bF0CdxSKbZUa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert length difference to a sparse matrix and concatenate with BoW features\n",
        "from scipy.sparse import csr_matrix\n",
        "length_diff_sparse = csr_matrix(data['length_diff']).transpose()\n",
        "\n",
        "X = hstack([X1, X2, length_diff_sparse])\n"
      ],
      "metadata": {
        "id": "1G2GZpFfbaPy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = data['is_duplicate']"
      ],
      "metadata": {
        "id": "j2JSvbLThrHF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Naive Bayes classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RPOYEDHbgA3",
        "outputId": "2f05b930-8465-4c67-d8cf-119b4dadc9ef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.79      0.78     50998\n",
            "           1       0.63      0.61      0.62     29873\n",
            "\n",
            "    accuracy                           0.73     80871\n",
            "   macro avg       0.71      0.70      0.70     80871\n",
            "weighted avg       0.72      0.73      0.73     80871\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of vectorizer"
      ],
      "metadata": {
        "id": "-btfzknWgSrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Train\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-wGcnc4hgsr",
        "outputId": "d974baee-b05e-4cc9-92cc-464b239ac8b4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.85      0.81     50998\n",
            "           1       0.68      0.56      0.62     29873\n",
            "\n",
            "    accuracy                           0.74     80871\n",
            "   macro avg       0.72      0.70      0.71     80871\n",
            "weighted avg       0.74      0.74      0.74     80871\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# List of text documents\n",
        "documents = [\"Cat cat sat on the mat\", \"The cat is named Fluffy\", \"Fluffy is not a cat but a dog\"]\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the model and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the result to an array\n",
        "print(X.toarray())\n",
        "\n",
        "# Get the feature names\n",
        "print(vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCmEXI4Kf1Ba",
        "outputId": "9ed00439-6014-4c37-a161-c4431cee560e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 2 0 0 0 1 0 0 1 1 1]\n",
            " [0 1 0 1 1 0 1 0 0 0 1]\n",
            " [1 1 1 1 1 0 0 1 0 0 0]]\n",
            "['but' 'cat' 'dog' 'fluffy' 'is' 'mat' 'named' 'not' 'on' 'sat' 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4j0zhdZqf1iX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}